{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafaelrpq/classificadores/blob/main/classificadores_ajustado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RjOg_4r2oxez",
        "outputId": "e90c15f1-4e5b-4584-af6f-5851db173740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando o dispositivo: cuda\n",
            "Downloading dataset...\n",
            "Path to dataset files: /home/rafaelquevedo/.cache/kagglehub/datasets/mudasirazhar/cyanotoxins-identification-dataset/versions/1\n",
            "Found Train and Test directories.\n",
            "Train dataset size: 2377\n",
            "Test dataset size: 595\n",
            "Classes: ['Anabaena', 'Aphanizomenon', 'Chroococcales', 'Cylindrospermopsis', 'Dolichospermum', 'Microcystis', 'Nostoc', 'Oscillatoria', 'Phormidium', 'Planktothrix', 'Raphidiopsis', 'Rivularia', 'Synechococcus']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vitb8 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded facebook/dino-vitb8 as AutoModel.\n",
            "Feature extractor model loaded and set to evaluation mode.\n",
            "\n",
            "============================================================\n",
            "--- Initial Feature Extraction (Full Train and Test) ---\n",
            "============================================================\n",
            "\n",
            "Using extractor: DINO\n",
            "Extracting features for 2377 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Features:   0%|          | 0/38 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 1.94 GiB of which 74.06 MiB is free. Process 17435 has 27.88 MiB memory in use. Process 2987 has 10.04 MiB memory in use. Including non-PyTorch memory, this process has 1.32 GiB memory in use. Of the allocated memory 1.23 GiB is allocated by PyTorch, and 45.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 249\u001b[39m\n\u001b[32m    246\u001b[39m full_train_loader_for_extraction = DataLoader(train_dataset, batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    247\u001b[39m test_loader_for_extraction = DataLoader(test_dataset, batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m X_train_full, y_train_full = \u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_train_loader_for_extraction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m X_test_full, y_test_full = extractor.extract_features(test_loader_for_extraction)\n\u001b[32m    252\u001b[39m feature_data[extractor_name] = {\n\u001b[32m    253\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mX_train_full\u001b[39m\u001b[33m'\u001b[39m: X_train_full,\n\u001b[32m    254\u001b[39m     \u001b[33m'\u001b[39m\u001b[33my_train_full\u001b[39m\u001b[33m'\u001b[39m: y_train_full,\n\u001b[32m    255\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mX_test_full\u001b[39m\u001b[33m'\u001b[39m: X_test_full,\n\u001b[32m    256\u001b[39m     \u001b[33m'\u001b[39m\u001b[33my_test_full\u001b[39m\u001b[33m'\u001b[39m: y_test_full,\n\u001b[32m    257\u001b[39m }\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36mFeatureExtractor.extract_features\u001b[39m\u001b[34m(self, dataloader)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc=\u001b[33m\"\u001b[39m\u001b[33mExtracting Features\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    133\u001b[39m     inputs = inputs.to (device)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Attempt to get the CLS token or pooled output\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[33m'\u001b[39m\u001b[33mlast_hidden_state\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m outputs.last_hidden_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    138\u001b[39m         \u001b[38;5;66;03m# ViT-like models often use the first token (CLS)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/transformers/models/vit/modeling_vit.py:554\u001b[39m, in \u001b[36mViTModel.forward\u001b[39m\u001b[34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[39m\n\u001b[32m    548\u001b[39m     pixel_values = pixel_values.to(expected_dtype)\n\u001b[32m    550\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    551\u001b[39m     pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding\n\u001b[32m    552\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    562\u001b[39m sequence_output = \u001b[38;5;28mself\u001b[39m.layernorm(sequence_output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/transformers/models/vit/modeling_vit.py:423\u001b[39m, in \u001b[36mViTEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    416\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    417\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    418\u001b[39m         hidden_states,\n\u001b[32m    419\u001b[39m         layer_head_mask,\n\u001b[32m    420\u001b[39m         output_attentions,\n\u001b[32m    421\u001b[39m     )\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    425\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/transformers/models/vit/modeling_vit.py:368\u001b[39m, in \u001b[36mViTLayer.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    364\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m    365\u001b[39m     head_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    366\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    367\u001b[39m ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayernorm_before\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# in ViT, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    374\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/transformers/models/vit/modeling_vit.py:310\u001b[39m, in \u001b[36mViTAttention.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    305\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    306\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m    307\u001b[39m     head_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    308\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    309\u001b[39m ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    314\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/transformers/models/vit/modeling_vit.py:242\u001b[39m, in \u001b[36mViTSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions)\u001b[39m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    240\u001b[39m         attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m context_layer, attention_probs = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m new_context_layer_shape = context_layer.size()[:-\u001b[32m2\u001b[39m] + (\u001b[38;5;28mself\u001b[39m.all_head_size,)\n\u001b[32m    254\u001b[39m context_layer = context_layer.reshape(new_context_layer_shape)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/python/classificadores/.venv/lib/python3.13/site-packages/transformers/integrations/sdpa_attention.py:40\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# SDPA with memory-efficient backend is bugged with non-contiguous inputs and custom attn_mask for some torch versions\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Reference: https://github.com/pytorch/pytorch/issues/112577.\u001b[39;00m\n\u001b[32m     39\u001b[39m query = query.contiguous()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m key = \u001b[43mkey\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m value = value.contiguous()\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Note that it is important to check first for the shape, otherwise compile will fail with `argument 'is_causal' must be bool, not SymBool`\u001b[39;00m\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 1.94 GiB of which 74.06 MiB is free. Process 17435 has 27.88 MiB memory in use. Process 2987 has 10.04 MiB memory in use. Including non-PyTorch memory, this process has 1.32 GiB memory in use. Of the allocated memory 1.23 GiB is allocated by PyTorch, and 45.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    balanced_accuracy_score, confusion_matrix\n",
        ")\n",
        "\n",
        "from transformers import AutoModel\n",
        "import warnings\n",
        "import kagglehub\n",
        "from tqdm import tqdm # For progress bars\n",
        "from collections import defaultdict # To help with aggregation\n",
        "\n",
        "# Import plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning) # For sklearn metrics zero_division\n",
        "\n",
        "# Configurar dispositivo (GPU se disponível)\n",
        "device = torch.device (\"cuda\" if torch.cuda.is_available () else \"cpu\")\n",
        "print (f\"Usando o dispositivo: {device}\")\n",
        "\n",
        "# Hiperparametros\n",
        "hyperparameters = {\n",
        "    \"SVC\": [{\"C\": c} for c in [0.1, 1, 10, 100, 1000]],\n",
        "    \"MLPClassifier\": [{\"hidden_layer_sizes\": hls, \"max_iter\" : 200, \"random_state\": 42} for hls in [(50,), (100,), (50, 50), (100, 100), (200, 200)]], # Added random_state\n",
        "    \"RandomForestClassifier\": [{\"n_estimators\": n, \"random_state\" : 42} for n in [10, 50, 100, 200, 500]],\n",
        "    \"KNeighborsClassifier\": [{\"n_neighbors\": k} for k in [1, 3, 5, 7, 9]]\n",
        "}\n",
        "\n",
        "# Transformação das imagens\n",
        "transform = transforms.Compose ([\n",
        "    transforms.Resize ((224, 224)),\n",
        "    transforms.ToTensor (),\n",
        "    transforms.Normalize (mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# --- Dataset Loading ---\n",
        "# Carregar o dataset usando kagglehub\n",
        "print(\"Downloading dataset...\")\n",
        "path = kagglehub.dataset_download(\"mudasirazhar/cyanotoxins-identification-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# !git clone https://github.com/iman2693/CTCB.git > /dev/null 2>&1\n",
        "# path = \"CTCB/dataset\"\n",
        "\n",
        "# Diretório das imagens\n",
        "train_data_dir = os.path.join (path,\"Dataset/Train\")\n",
        "test_data_dir = os.path.join (path,\"Dataset/Test\") # Assuming a 'Test' directory exists based on common dataset structures\n",
        "\n",
        "# Carregar os conjuntos de dados de treino e teste (se houver diretórios separados)\n",
        "# Se não houver Test directory, use train_test_split on the single directory\n",
        "if not os.path.exists(test_data_dir):\n",
        "     print(\"Test directory not found. Splitting train directory.\")\n",
        "     full_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
        "     test_ratio = 0.2\n",
        "     # Use indices for splitting to avoid issues with ImageFolder and Subset later\n",
        "     train_indices, test_indices = train_test_split(list(range(len(full_dataset))),\n",
        "                                                    test_size=test_ratio,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=full_dataset.targets)\n",
        "     train_dataset = Subset(full_dataset, train_indices)\n",
        "     test_dataset = Subset(full_dataset, test_indices)\n",
        "     # Need to get class names from the original full_dataset\n",
        "     class_names = full_dataset.classes\n",
        "     num_classes = len(class_names)\n",
        "\n",
        "else:\n",
        "    print(\"Found Train and Test directories.\")\n",
        "    train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
        "    test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n",
        "    class_names = train_dataset.classes # Assume train_dataset has all classes\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(f\"Classes: {class_names}\")\n",
        "\n",
        "\n",
        "# Dataloaders para a extração inicial de features do conjunto COMPLETO de treino e teste\n",
        "# Shuffle False for feature extraction to maintain order potentially, though not strictly necessary\n",
        "# Use a larger batch size for extraction if memory allows\n",
        "full_train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# KFold setup (apenas para o conjunto de TREINO inicial)\n",
        "k_folds = 5\n",
        "# Note: kf.split will return indices relative to the train_dataset subset if it's a Subset\n",
        "# If it's the original ImageFolder, it will return indices relative to that.\n",
        "# Our feature extraction step flattened this, so we need to be careful.\n",
        "# Since we extract features *before* KFold, KFold should split indices of the *extracted features*.\n",
        "# The number of samples for KFold should be len(X_train_full).\n",
        "kf = KFold (n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "# --- Feature Extraction Class ---\n",
        "class FeatureExtractor :\n",
        "    def __init__ (self, vit_model_name) :\n",
        "        try:\n",
        "             # Assuming the model is suitable for feature extraction with AutoModel\n",
        "             self.model = AutoModel.from_pretrained(vit_model_name).to(device)\n",
        "             print(f\"Loaded {vit_model_name} as AutoModel.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error loading {vit_model_name} with AutoModel: {e}\")\n",
        "             print(\"Ensure the model checkpoint is compatible or try a different loading strategy.\")\n",
        "             raise e\n",
        "\n",
        "        self.model.eval ()\n",
        "        print(f\"Feature extractor model loaded and set to evaluation mode.\")\n",
        "\n",
        "    def extract_features (self, dataloader):\n",
        "        features = []\n",
        "        labels = [] # Also return labels to keep them aligned with features\n",
        "        print(f\"Extracting features for {len(dataloader.dataset)} samples...\")\n",
        "        with torch.no_grad ():\n",
        "            for inputs, targets in tqdm(dataloader, desc=\"Extracting Features\"):\n",
        "                inputs = inputs.to (device)\n",
        "                outputs = self.model (inputs)\n",
        "\n",
        "                # Attempt to get the CLS token or pooled output\n",
        "                if hasattr(outputs, 'last_hidden_state') and outputs.last_hidden_state is not None:\n",
        "                    # ViT-like models often use the first token (CLS)\n",
        "                    cls_tokens = outputs.last_hidden_state[:, 0, :]\n",
        "                elif hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "                     # Some models provide a pooled output\n",
        "                     cls_tokens = outputs.pooler_output\n",
        "                else:\n",
        "                    # Fallback - might need adjustment based on specific model output\n",
        "                    # print(f\"Warning: Could not find standard feature outputs for {self.model.__class__.__name__}. Using first output tensor's first token.\")\n",
        "                    try:\n",
        "                        # This is a risky fallback, verify for your model\n",
        "                        cls_tokens = outputs[0][:, 0, :]\n",
        "                    except Exception as e:\n",
        "                         print(f\"Error accessing fallback output: {e}\")\n",
        "                         print(\"Model output structure is unexpected. Check model documentation.\")\n",
        "                         raise e\n",
        "\n",
        "\n",
        "                features.append (cls_tokens.cpu ().numpy ())  # Move to CPU and convert to numpy\n",
        "                labels.append (targets.numpy ())\n",
        "\n",
        "        return np.vstack (features), np.hstack (labels)\n",
        "\n",
        "\n",
        "# --- Evaluation Function ---\n",
        "def evaluate_metrics(y_true, y_pred, class_names=None, num_classes=None):\n",
        "    \"\"\"Calculates various classification metrics and CM components.\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Handle cases with single class or limited classes in a subset/fold\n",
        "    unique_classes_true = np.unique(y_true)\n",
        "    unique_classes_pred = np.unique(y_pred)\n",
        "    combined_unique_classes = np.unique(np.concatenate((unique_classes_true, unique_classes_pred)))\n",
        "\n",
        "    precision, recall, f1 = np.nan, np.nan, np.nan\n",
        "    if len(combined_unique_classes) >= 2:\n",
        "        # Cannot calculate macro average if less than two unique classes are present in combined true/pred\n",
        "        try:\n",
        "            precision = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "            recall = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "            f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "        except ValueError as e:\n",
        "             # This might happen if one class has no true samples but predictions, etc.\n",
        "             # print(f\"Warning: Error calculating macro metrics: {e}. Setting to NaN.\") # Too verbose\n",
        "             precision, recall, f1 = np.nan, np.nan, np.nan\n",
        "\n",
        "\n",
        "    balanced = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Confusion Matrix and per-class TP/TN/FP/FN\n",
        "    # Use labels argument to ensure matrix shape corresponds to all possible classes\n",
        "    all_possible_labels = np.arange(num_classes) if num_classes is not None else np.unique(np.concatenate((y_true, y_pred)))\n",
        "    # Ensure labels array is sorted if you're passing it explicitly\n",
        "    all_possible_labels = np.sort(all_possible_labels)\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=all_possible_labels)\n",
        "\n",
        "    n_classes_cm = cm.shape[0]\n",
        "    cm_metrics_per_class = {}\n",
        "\n",
        "    # Ensure class names align with matrix indices\n",
        "    current_class_names = class_names if class_names is not None and len(class_names) == n_classes_cm else [f'class_{i}' for i in range(n_classes_cm)]\n",
        "\n",
        "    for i in range(n_classes_cm):\n",
        "        class_label = current_class_names[i]\n",
        "        # For class i (index i):\n",
        "        TP_i = cm[i, i] # True class i, Predicted class i\n",
        "        FN_i = np.sum(cm[i, :]) - TP_i # True class i, Predicted NOT class i (sum of row i excluding diagonal)\n",
        "        FP_i = np.sum(cm[:, i]) - TP_i # True NOT class i, Predicted class i (sum of col i excluding diagonal)\n",
        "        TN_i = np.sum(cm) - TP_i - FN_i - FP_i # True NOT class i, Predicted NOT class i (total - others)\n",
        "\n",
        "        cm_metrics_per_class[class_label] = {'TP': int(TP_i), 'FN': int(FN_i), 'FP': int(FP_i), 'TN': int(TN_i)} # Ensure int\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": float(acc), # Ensure float\n",
        "        \"precision_macro\": float(precision) if precision is not np.nan else None,\n",
        "        \"recall_macro\": float(recall) if recall is not np.nan else None,\n",
        "        \"f1_macro\": float(f1) if f1 is not np.nan else None,\n",
        "        \"balanced_accuracy\": float(balanced),\n",
        "        \"confusion_matrix\": cm.tolist(), # Store as list\n",
        "        \"cm_metrics_per_class\": cm_metrics_per_class\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "# Instanciando Extratores de Caracteristicas baseados em ViT Models\n",
        "vit_extractors = {\n",
        "    \"DINO\": FeatureExtractor(\"facebook/dino-vitb8\"),\n",
        "    # \"ViT-Base\": FeatureExtractor(\"google/vit-base-patch16-224\"), # Uncomment to include\n",
        "    # \"ViT-Large\": FeatureExtractor(\"google/vit-large-patch16-224\"), # Uncomment to include\n",
        "}\n",
        "\n",
        "# Instanciando Classificadores (using sklearn class objects directly)\n",
        "classifiers_map = {\n",
        "    'SVM': SVC,\n",
        "    'MLP': MLPClassifier,\n",
        "    'RandomForest': RandomForestClassifier,\n",
        "    'KNN': KNeighborsClassifier,\n",
        "}\n",
        "\n",
        "# --- 1. Extract features from the FULL train and test datasets ONCE ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- Initial Feature Extraction (Full Train and Test) ---\")\n",
        "print(\"=\"*60)\n",
        "feature_data = {} # Store features and labels per extractor\n",
        "\n",
        "# Use train_dataset and test_dataset directly for feature extraction\n",
        "for extractor_name, extractor in vit_extractors.items():\n",
        "    print(f\"\\nUsing extractor: {extractor_name}\")\n",
        "    full_train_loader_for_extraction = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
        "    test_loader_for_extraction = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    X_train_full, y_train_full = extractor.extract_features(full_train_loader_for_extraction)\n",
        "    X_test_full, y_test_full = extractor.extract_features(test_loader_for_extraction)\n",
        "\n",
        "    feature_data[extractor_name] = {\n",
        "        'X_train_full': X_train_full,\n",
        "        'y_train_full': y_train_full,\n",
        "        'X_test_full': X_test_full,\n",
        "        'y_test_full': y_test_full,\n",
        "    }\n",
        "    print(f\"Extracted features shape (Train): {X_train_full.shape}\")\n",
        "    print(f\"Extracted features shape (Test): {X_test_full.shape}\")\n",
        "\n",
        "# --- 2. K-Fold Cross-Validation on Training Data ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- Starting K-Fold Cross-Validation on Training Data ---\")\n",
        "print(\"=\"*60)\n",
        "kfold_results = [] # Store results for each fold, classifier, and params\n",
        "\n",
        "# K-Fold split on the indices of the extracted full training features\n",
        "for extractor_name, data in feature_data.items():\n",
        "    X_train_full = data['X_train_full']\n",
        "    y_train_full = data['y_train_full']\n",
        "\n",
        "    print(f\"\\n--- K-Fold for Extractor: {extractor_name} ---\")\n",
        "\n",
        "    # kf.split splits indices 0..N-1 where N is the first argument's length\n",
        "    for fold_idx, (train_indices_fold, val_indices_fold) in enumerate(kf.split(X_train_full)):\n",
        "        print(f\"\\nProcessing Fold {fold_idx + 1}/{k_folds}\")\n",
        "\n",
        "        # Get the features and labels for this fold using the indices\n",
        "        X_train_fold = X_train_full[train_indices_fold]\n",
        "        y_train_fold = y_train_full[train_indices_fold]\n",
        "        X_val_fold = X_train_full[val_indices_fold]\n",
        "        y_val_fold = y_train_full[val_indices_fold]\n",
        "\n",
        "        # Iterate through each classifier\n",
        "        for clf_name, clf_class in classifiers_map.items():\n",
        "            clf_hyperparameters = hyperparameters.get(clf_class.__name__, [])\n",
        "\n",
        "            if not clf_hyperparameters:\n",
        "                 continue\n",
        "\n",
        "            for params in tqdm(clf_hyperparameters, desc=f\"  Fold {fold_idx+1} {clf_name} Params\"):\n",
        "                try:\n",
        "                    clf = clf_class(**params)\n",
        "                    clf.fit(X_train_fold, y_train_fold)\n",
        "                    y_pred_fold = clf.predict(X_val_fold)\n",
        "                    metrics = evaluate_metrics(y_val_fold, y_pred_fold, class_names=class_names, num_classes=num_classes)\n",
        "\n",
        "                    kfold_results.append({\n",
        "                        'extractor': extractor_name,\n",
        "                        'fold_idx': fold_idx + 1,\n",
        "                        'classifier': clf_name,\n",
        "                        'params': params,\n",
        "                        'metrics': metrics\n",
        "                        # Note: y_true/y_pred not strictly needed for selection, saving memory\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                     print(f\"\\nError training/predicting {clf_name} with params {params} in fold {fold_idx+1}: {e}\")\n",
        "                     kfold_results.append({\n",
        "                         'extractor': extractor_name,\n",
        "                         'fold_idx': fold_idx + 1,\n",
        "                         'classifier': clf_name,\n",
        "                         'params': params,\n",
        "                         'error': str(e)\n",
        "                     })\n",
        "\n",
        "print(\"\\n--- K-Fold Cross-Validation Finished ---\")\n",
        "print(f\"Total K-Fold results stored (including errors): {len(kfold_results)}\")\n",
        "\n",
        "\n",
        "# --- 3. Analyze K-Fold Results and Select Best Hyperparameters ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- Analyzing K-Fold Results and Selecting Best Models ---\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "aggregated_kfold_metrics = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "for result in kfold_results:\n",
        "    if 'error' in result:\n",
        "        continue\n",
        "\n",
        "    extractor = result['extractor']\n",
        "    classifier = result['classifier']\n",
        "    # Convert params dict to a hashable format (string representation)\n",
        "    # Use json.dumps for a more stable representation than str(), handling dict order\n",
        "    try:\n",
        "         import json\n",
        "         params_key = json.dumps(result['params'], sort_keys=True)\n",
        "    except:\n",
        "         params_key = str(result['params']) # Fallback if json fails\n",
        "\n",
        "    for metric_name, metric_value in result['metrics'].items():\n",
        "        if isinstance(metric_value, (int, float)) or (metric_value is None):\n",
        "             if metric_value is not None: # Only average non-None values\n",
        "                aggregated_kfold_metrics[(extractor, classifier, params_key)][metric_name].append(metric_value)\n",
        "\n",
        "\n",
        "average_kfold_metrics = {}\n",
        "best_params_per_clf_extractor = {}\n",
        "\n",
        "selection_metric = 'balanced_accuracy' # Metric used to select the best hyperparameters\n",
        "\n",
        "print(f\"Selecting best hyperparameters based on average '{selection_metric}' across folds.\")\n",
        "\n",
        "# Need to map params_key back to original dict structure\n",
        "# Create a mapping from the string representation to the actual dictionary\n",
        "params_key_to_dict = {}\n",
        "for res in kfold_results:\n",
        "    if 'error' not in res:\n",
        "        try:\n",
        "            import json\n",
        "            params_key_json = json.dumps(res['params'], sort_keys=True)\n",
        "            params_key_to_dict[params_key_json] = res['params']\n",
        "        except:\n",
        "            params_key_str = str(res['params']) # Fallback if json fails\n",
        "            params_key_to_dict[params_key_str] = res['params']\n",
        "\n",
        "\n",
        "for (extractor, classifier, params_key), metrics_list_dict in aggregated_kfold_metrics.items():\n",
        "    average_metrics = {\n",
        "        metric_name: np.nanmean(metric_values)\n",
        "        for metric_name, metric_values in metrics_list_dict.items()\n",
        "    }\n",
        "\n",
        "    # average_kfold_metrics is now correctly keyed by (extractor, classifier, params_key)\n",
        "    average_kfold_metrics[(extractor, classifier, params_key)] = average_metrics\n",
        "\n",
        "    current_best_info = best_params_per_clf_extractor.get((extractor, classifier))\n",
        "    current_combination_score = average_metrics.get(selection_metric, np.nan) # Get score, default to NaN if metric missing\n",
        "\n",
        "    # Update best if current score is better (handling NaN: NaN is never > a number, nor is a number > NaN. Treat NaN as worst.)\n",
        "    if current_best_info is None or np.isnan(current_best_info['avg_score']) or (not np.isnan(current_combination_score) and current_combination_score > current_best_info['avg_score']):\n",
        "        # Retrieve the original params dict using the params_key\n",
        "        original_params = params_key_to_dict.get(params_key)\n",
        "        best_params_per_clf_extractor[(extractor, classifier)] = {\n",
        "            'params': original_params, # Store the actual params dict here\n",
        "            'avg_score': current_combination_score\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"\\n--- K-Fold Average Metrics Summary (Best per Classifier) ---\")\n",
        "# Sort by score, treat NaN as a low value (-1 or negative infinity) for sorting\n",
        "sorted_best_kfold = sorted(best_params_per_clf_extractor.items(),\n",
        "                           key=lambda item: item[1]['avg_score'] if not np.isnan(item[1]['avg_score']) else -float('inf'),\n",
        "                           reverse=True)\n",
        "\n",
        "for (extractor, classifier), best_info in sorted_best_kfold:\n",
        "     best_params = best_info['params']\n",
        "     avg_score = best_info['avg_score']\n",
        "\n",
        "     # === FIX ===\n",
        "     # Use the params_key (string representation) to lookup in average_kfold_metrics\n",
        "     try:\n",
        "         import json\n",
        "         best_params_key = json.dumps(best_params, sort_keys=True)\n",
        "     except:\n",
        "         best_params_key = str(best_params)\n",
        "\n",
        "     full_avg_metrics = average_kfold_metrics.get((extractor, classifier, best_params_key), {})\n",
        "     # === END FIX ===\n",
        "\n",
        "     print(f\"\\nBest K-Fold Result for Extractor: {extractor}, Classifier: {classifier}\")\n",
        "     print(f\"  Selected Params: {best_params}\")\n",
        "     print(f\"  Avg {selection_metric}: {avg_score:.4f}\" if not np.isnan(avg_score) else f\"  Avg {selection_metric}: N/A (all folds failed or metric NaN)\")\n",
        "     # Safely print average metrics, handling potential missing keys gracefully\n",
        "     print(f\"  Avg Metrics: Accuracy={full_avg_metrics.get('accuracy', np.nan):.4f}, Precision={full_avg_metrics.get('precision_macro', np.nan):.4f}, Recall={full_avg_metrics.get('recall_macro', np.nan):.4f}, F1={full_avg_metrics.get('f1_macro', np.nan):.4f}\")\n",
        "\n",
        "\n",
        "# --- 4. Final Evaluation on the Test Set (Using Selected Best Hyperparameters) ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- Starting Final Evaluation on Test Set (Using Selected Best Models) ---\")\n",
        "print(\"=\"*60)\n",
        "final_test_results = []\n",
        "\n",
        "# For each extractor (only DINO in this run)\n",
        "for extractor_name, data in feature_data.items():\n",
        "    X_train_full = data['X_train_full']\n",
        "    y_train_full = data['y_train_full']\n",
        "    X_test_full = data['X_test_full']\n",
        "    y_test_full = data['y_test_full']\n",
        "\n",
        "    print(f\"\\n--- Testing with Extractor: {extractor_name} ---\")\n",
        "\n",
        "    # For each classifier, get the best params found\n",
        "    for clf_name, clf_class in classifiers_map.items():\n",
        "         best_info = best_params_per_clf_extractor.get((extractor_name, clf_name))\n",
        "\n",
        "         # Also check if the best score found was NaN, if so, maybe skip testing this config\n",
        "         if best_info is None or best_info['params'] is None or np.isnan(best_info.get('avg_score', np.nan)):\n",
        "              print(f\"  No valid best parameters found for {clf_name} with {extractor_name} (or avg score was NaN). Skipping final test.\")\n",
        "              continue\n",
        "\n",
        "         params = best_info['params']\n",
        "         print(f\"  Testing {clf_name} with Selected Best Params: {params}\")\n",
        "\n",
        "         try:\n",
        "            # Instantiate and train the classifier on the *entire* initial training set\n",
        "            clf = clf_class(**params)\n",
        "            clf.fit(X_train_full, y_train_full)\n",
        "\n",
        "            # Predict on the held-out test set\n",
        "            y_pred_test = clf.predict(X_test_full)\n",
        "\n",
        "            # Evaluate metrics for the final test set\n",
        "            test_metrics = evaluate_metrics(y_test_full, y_pred_test, class_names=class_names, num_classes=num_classes)\n",
        "\n",
        "            # Store final test results (only one entry per extractor/classifier now)\n",
        "            final_test_results.append({\n",
        "                'extractor': extractor_name,\n",
        "                'classifier': clf_name,\n",
        "                'params': params, # Store the best params used\n",
        "                'metrics': test_metrics,\n",
        "                'y_true': y_test_full.tolist(), # Keep for CM plotting\n",
        "                'y_pred': y_pred_test.tolist() # Keep for CM plotting\n",
        "            })\n",
        "         except Exception as e:\n",
        "             print(f\"\\nError training/predicting selected {clf_name} with params {params} on test set: {e}\")\n",
        "             final_test_results.append({\n",
        "                 'extractor': extractor_name,\n",
        "                 'classifier': clf_name,\n",
        "                 'params': params,\n",
        "                 'error': str(e)\n",
        "             })\n",
        "\n",
        "\n",
        "print(\"\\n--- Final Evaluation on Test Set Finished ---\")\n",
        "print(f\"Total Final Test results stored (selected models): {len(final_test_results)}\")\n",
        "\n",
        "\n",
        "# --- 5. Display Final Test Results (with Plotting) ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- Summarizing Final Test Results (Selected Models) ---\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if not final_test_results:\n",
        "    print(\"No final test results to display.\")\n",
        "else:\n",
        "    for result in final_test_results:\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"Extractor: {result.get('extractor', 'N/A')}\")\n",
        "        print(f\"Classifier: {result.get('classifier', 'N/A')}\")\n",
        "        print(f\"Parameters (Selected by K-Fold): {result.get('params', 'N/A')}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        if 'error' in result:\n",
        "             print(f\"Error during final test: {result['error']}\")\n",
        "             continue\n",
        "\n",
        "        metrics = result['metrics']\n",
        "\n",
        "        # --- Print Scalar Metrics ---\n",
        "        print(f\"  Metrics on Test Set:\")\n",
        "        # Use .get() and handle None/NaN gracefully\n",
        "        acc = metrics.get('accuracy')\n",
        "        bal_acc = metrics.get('balanced_accuracy')\n",
        "        prec = metrics.get('precision_macro')\n",
        "        rec = metrics.get('recall_macro')\n",
        "        f1_m = metrics.get('f1_macro')\n",
        "\n",
        "        print(f\"    Accuracy: {acc:.4f}\" if acc is not None else \"    Accuracy: N/A\")\n",
        "        print(f\"    Balanced Accuracy: {bal_acc:.4f}\" if bal_acc is not None else \"    Balanced Accuracy: N/A\")\n",
        "        print(f\"    Precision (macro): {prec:.4f}\" if prec is not None else \"    Precision (macro): N/A\")\n",
        "        print(f\"    Recall (macro): {rec:.4f}\" if rec is not None else \"    Recall (macro): N/A\")\n",
        "        print(f\"    F1-Score (macro): {f1_m:.4f}\" if f1_m is not None else \"    F1-Score (macro): N/A\")\n",
        "\n",
        "\n",
        "        # --- Plot Confusion Matrix ---\n",
        "        cm_list = metrics.get('confusion_matrix')\n",
        "        if cm_list is not None and class_names is not None: # Ensure class_names are available for plotting\n",
        "            cm_np = np.array(cm_list)\n",
        "\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            sns.heatmap(cm_np,\n",
        "                        annot=True, fmt='d', cmap='Blues',\n",
        "                        xticklabels=class_names, yticklabels=class_names)\n",
        "            plt.xlabel('Predicted Label')\n",
        "            plt.ylabel('True Label')\n",
        "            # Include params in the title, maybe as a string summary\n",
        "            # Use the actual params dict which is stored in the result\n",
        "            params_dict_for_title = result.get('params', {})\n",
        "            params_str_title = \", \".join([f\"{k}={v}\" for k, v in params_dict_for_title.items()])\n",
        "\n",
        "            plt.title(f'Confusion Matrix: {result[\"classifier\"]} ({result[\"extractor\"]})\\nParams: {params_str_title}')\n",
        "            plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "            plt.show() # Display the plot\n",
        "        elif cm_list is not None:\n",
        "             print(\"\\n  Confusion Matrix (cannot plot without class names):\")\n",
        "             print(np.array(cm_list))\n",
        "\n",
        "\n",
        "        # --- Print Per-Class TP/TN/FP/FN ---\n",
        "        cm_metrics_per_class = metrics.get('cm_metrics_per_class')\n",
        "        if cm_metrics_per_class:\n",
        "            print(\"\\n  Per-Class TP/TN/FP/FN on Test Set:\")\n",
        "            sorted_class_labels = sorted(cm_metrics_per_class.keys()) # Sort by class name/key\n",
        "            for class_label in sorted_class_labels:\n",
        "                cm_vals = cm_metrics_per_class[class_label]\n",
        "                tp = cm_vals.get('TP', 'N/A')\n",
        "                tn = cm_vals.get('TN', 'N/A')\n",
        "                fp = cm_vals.get('FP', 'N/A')\n",
        "                fn = cm_vals.get('FN', 'N/A')\n",
        "                print(f\"    {class_label}: TP={tp}, TN={tn}, FP={fp}, FN={fn}\")\n",
        "\n",
        "        print(\"=\" * 60) # Separator for next result\n",
        "\n",
        "print(\"\\n--- All Final Test Results Displayed ---\")\n",
        "\n",
        "# --- 6. Identify and Display Overall Best Result ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- Overall Best Result on Test Set ---\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "best_result_entry = None\n",
        "highest_balanced_accuracy = -1.0 # Initialize with a value lower than any possible score\n",
        "\n",
        "# Iterate through the final test results to find the best one\n",
        "for result in final_test_results:\n",
        "    # Skip results with errors or missing metrics\n",
        "    if 'error' in result or result.get('metrics') is None:\n",
        "        continue\n",
        "\n",
        "    current_balanced_accuracy = result['metrics'].get('balanced_accuracy')\n",
        "\n",
        "    # Compare, handling None values (None is treated as worse than any number)\n",
        "    if current_balanced_accuracy is not None:\n",
        "        if best_result_entry is None or current_balanced_accuracy > highest_balanced_accuracy:\n",
        "            highest_balanced_accuracy = current_balanced_accuracy\n",
        "            best_result_entry = result\n",
        "\n",
        "# Print the details of the best result found\n",
        "if best_result_entry:\n",
        "    print(f\"\\nOverall Best Model configuration based on Test Set Balanced Accuracy ({highest_balanced_accuracy:.4f}):\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Extractor: {best_result_entry['extractor']}\")\n",
        "    print(f\"Classifier: {best_result_entry['classifier']}\")\n",
        "    print(f\"Parameters (Selected by K-Fold): {best_result_entry['params']}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(\"Metrics on Test Set for the Best Model:\")\n",
        "\n",
        "    metrics = best_result_entry['metrics']\n",
        "    acc = metrics.get('accuracy')\n",
        "    bal_acc = metrics.get('balanced_accuracy')\n",
        "    prec = metrics.get('precision_macro')\n",
        "    rec = metrics.get('recall_macro')\n",
        "    f1_m = metrics.get('f1_macro')\n",
        "\n",
        "    print(f\"  Accuracy: {acc:.4f}\" if acc is not None else \"  Accuracy: N/A\")\n",
        "    print(f\"  Balanced Accuracy: {bal_acc:.4f}\" if bal_acc is not None else \"  Balanced Accuracy: N/A\")\n",
        "    print(f\"  Precision (macro): {prec:.4f}\" if prec is not None else \"  Precision (macro): N/A\")\n",
        "    print(f\"  Recall (macro): {rec:.4f}\" if rec is not None else \"  Recall (macro): N/A\")\n",
        "    print(f\"  F1-Score (macro): {f1_m:.4f}\" if f1_m is not None else \"  F1-Score (macro): N/A\")\n",
        "\n",
        "    # You could optionally re-plot the CM or print CM details here if desired,\n",
        "    # but it's often sufficient to just show the scalar metrics for the \"best\".\n",
        "\n",
        "else:\n",
        "    print(\"Could not determine the overall best result (perhaps all runs failed or had no valid metrics).\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPxR0tsQWbXDvw9zOAB5eDe",
      "gpuType": "A100",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPcZNFyeVlAuKCOkMCU2QwU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafaelrpq/classificadores/blob/main/classificadores_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "from efc import EnergyBasedFlowClassifier\n",
        "\n",
        "from transformers import AutoModel\n",
        "import warnings\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Configurar dispositivo (GPU se disponível)\n",
        "device = torch.device (\"cuda\" if torch.cuda.is_available () else \"cpu\")\n",
        "print (f\"Usando o dispositivo: {device}\")\n",
        "\n",
        "# Hiperparametros\n",
        "# Hiperparametros\n",
        "hyperparameters = {\n",
        "    \"SVC\": [{\"C\": c} for c in [0.1, 1, 10, 100, 1000]],\n",
        "    \"MLPClassifier\": [{\"hidden_layer_sizes\": hls, \"max_iter\" : 200, \"random_state\": 42} for hls in [(50,), (100,), (50, 50), (100, 100), (200, 200)]],\n",
        "    \"RandomForestClassifier\": [{\"n_estimators\": n, \"random_state\" : 42} for n in [10, 50, 100, 200, 500]],\n",
        "    \"KNeighborsClassifier\": [{\"n_neighbors\": k} for k in [1, 3, 5, 7, 9]],\n",
        "    \"EnergyBasedFlowClassifier\": [\n",
        "        {\"n_epochs\": 20, \"learning_rate\": 1e-3, \"verbose\": 0},\n",
        "        {\"n_epochs\": 50, \"learning_rate\": 1e-3, \"verbose\": 0},\n",
        "        {\"n_epochs\": 50, \"learning_rate\": 5e-4, \"verbose\": 0}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Transformação das imagens\n",
        "transform = transforms.Compose ([\n",
        "    transforms.Resize ((224, 224)),\n",
        "    transforms.ToTensor (),\n",
        "    transforms.Normalize (mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Carrega o dataset\n",
        "print(\"Downloading dataset...\")\n",
        "!git clone https://github.com/iman2693/CTCB.git > /dev/null 2>&1\n",
        "path = \"/content/CTCB\"\n",
        "\n",
        "\n",
        "# Diretório das imagens\n",
        "train_data_dir = os.path.join (path,\"dataset/Train\")\n",
        "# Assumindo que um diretório 'Test' existe baseda na estrutra comum dos datasets\n",
        "test_data_dir = os.path.join (path,\"dataset/Test\")\n",
        "\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n",
        "class_names = train_dataset.classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(f\"Classes: {class_names}\")\n",
        "print(f\"Number of classes: {num_classes}\") # Explicitly print num_classes\n",
        "\n",
        "\n",
        "# KFold setup (apenas para o conjunto de TREINO inicial)\n",
        "k_folds = 5\n",
        "kf = KFold (n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "class FeatureExtractor :\n",
        "    def __init__ (self, vit_model_name) :\n",
        "        try:\n",
        "             self.model = AutoModel.from_pretrained(vit_model_name).to(device)\n",
        "             print(f\"Loaded {vit_model_name} as AutoModel.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error loading {vit_model_name} with AutoModel: {e}\")\n",
        "             print(\"Ensure the model checkpoint is compatible or try a different loading strategy.\")\n",
        "             raise e\n",
        "\n",
        "        self.model.eval ()\n",
        "        print(f\"Feature extractor model loaded and set to evaluation mode.\")\n",
        "\n",
        "    def extract_features (self, dataloader):\n",
        "        features = []\n",
        "        labels = []\n",
        "        print(f\"Extracting features for {len(dataloader.dataset)} samples...\")\n",
        "        with torch.no_grad ():\n",
        "            for inputs, targets in tqdm(dataloader, desc=\"Extracting Features\"):\n",
        "                inputs = inputs.to (device)\n",
        "                outputs = self.model (inputs)\n",
        "\n",
        "                if hasattr(outputs, 'last_hidden_state') and outputs.last_hidden_state is not None:\n",
        "                    cls_tokens = outputs.last_hidden_state[:, 0, :]\n",
        "                elif hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "                    cls_tokens = outputs.pooler_output\n",
        "                else:\n",
        "                    try:\n",
        "                         cls_tokens = outputs[0][:, 0, :]\n",
        "                    except Exception as e:\n",
        "                         print(f\"Error accessing fallback output structure: {e}\")\n",
        "                         print(\"Model output structure is unexpected. Check model documentation or model type.\")\n",
        "                         raise e\n",
        "\n",
        "\n",
        "                # Move para CPU e converte para numpy\n",
        "                features.append (cls_tokens.cpu ().numpy ())\n",
        "                labels.append (targets.numpy ())\n",
        "\n",
        "        return np.vstack (features), np.hstack (labels)\n",
        "\n",
        "\n",
        "def evaluate_metrics(y_true, y_pred, class_names=None, num_classes=None):\n",
        "    \"\"\"Calculates various classification metrics (including macro and weighted) and CM components.\"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "\n",
        "    unique_labels, counts = np.unique(y_true, return_counts=True)\n",
        "\n",
        "    weights = 1.0 / counts\n",
        "\n",
        "    class_weight_map = {label: weight for label, weight in zip(unique_labels, weights)}\n",
        "\n",
        "    sample_weights = np.array([class_weight_map.get(label, 0.0) for label in y_true])\n",
        "\n",
        "    if np.sum(sample_weights) > 0:\n",
        "       acc_weighted_sample = accuracy_score(y_true, y_pred, sample_weight=sample_weights)\n",
        "    else:\n",
        "       print(\"Warning: Cannot calculate sample-weighted accuracy, sum of sample weights is zero.\")\n",
        "       acc_weighted_sample = np.nan\n",
        "\n",
        "\n",
        "    unique_classes_true = np.unique(y_true)\n",
        "    unique_classes_pred = np.unique(y_pred)\n",
        "    combined_unique_classes = np.unique(np.concatenate((unique_classes_true, unique_classes_pred)))\n",
        "\n",
        "    precision_macro, recall_macro, f1_macro = np.nan, np.nan, np.nan\n",
        "    precision_weighted, recall_weighted, f1_weighted = np.nan, np.nan, np.nan\n",
        "\n",
        "\n",
        "    if len(combined_unique_classes) >= 2:\n",
        "        try:\n",
        "            precision_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "            recall_macro = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "            f1_macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "\n",
        "            precision_weighted = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
        "            recall_weighted = recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
        "            f1_weighted = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
        "\n",
        "        except ValueError as e:\n",
        "             print(f\"Warning: Could not calculate average metrics (macro/weighted) due to ValueError: {e}\")\n",
        "             pass\n",
        "\n",
        "\n",
        "    all_possible_labels = np.arange(num_classes) if num_classes is not None else np.unique(np.concatenate((y_true, y_pred)))\n",
        "    all_possible_labels = np.sort(all_possible_labels)\n",
        "\n",
        "    try:\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=all_possible_labels)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error calculating confusion matrix: {e}\")\n",
        "        print(f\"y_true unique: {np.unique(y_true)}\")\n",
        "        print(f\"y_pred unique: {np.unique(y_pred)}\")\n",
        "        print(f\"all_possible_labels: {all_possible_labels}\")\n",
        "        cm = np.array([])\n",
        "\n",
        "\n",
        "    n_classes_cm = cm.shape[0] if cm.ndim == 2 else 0\n",
        "    cm_metrics_per_class = {}\n",
        "\n",
        "    if n_classes_cm > 0:\n",
        "        current_class_names = class_names if class_names is not None and len(class_names) == n_classes_cm else [f'class_{i}' for i in range(n_classes_cm)]\n",
        "\n",
        "        total_samples = np.sum(cm)\n",
        "        for i in range(n_classes_cm):\n",
        "            class_label = current_class_names[i]\n",
        "\n",
        "            TP_i = cm[i, i]\n",
        "            FN_i = np.sum(cm[i, :]) - TP_i\n",
        "            FP_i = np.sum(cm[:, i]) - TP_i\n",
        "            TN_i = total_samples - TP_i - FN_i - FP_i\n",
        "\n",
        "            cm_metrics_per_class[class_label] = {'TP': int(TP_i), 'FN': int(FN_i), 'FP': int(FP_i), 'TN': int(TN_i)}\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": float(acc_weighted_sample) if not np.isnan(acc_weighted_sample) else None,\n",
        "        \"precision_macro\": float(precision_macro) if precision_macro is not np.nan else None,\n",
        "        \"recall_macro\": float(recall_macro) if recall_macro is not np.nan else None,\n",
        "        \"f1_macro\": float(f1_macro) if f1_macro is not np.nan else None,\n",
        "        \"precision_weighted\": float(precision_weighted) if precision_weighted is not np.nan else None,\n",
        "        \"recall_weighted\": float(recall_weighted) if recall_weighted is not np.nan else None,\n",
        "        \"f1_weighted\": float(f1_weighted) if f1_weighted is not np.nan else None,\n",
        "        \"confusion_matrix\": cm.tolist() if cm.ndim == 2 else None,\n",
        "        \"cm_metrics_per_class\": cm_metrics_per_class\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# Instanciando Extratores de Caracteristicas baseados em ViT Models\n",
        "vit_extractors = {\n",
        "    \"DINO\": FeatureExtractor(\"facebook/dino-vitb8\"),\n",
        "    \"ViT-Base\": FeatureExtractor(\"google/vit-base-patch16-224\"),\n",
        "    \"ViT-Large\": FeatureExtractor(\"google/vit-large-patch16-224\"),\n",
        "}\n",
        "\n",
        "# Instanciando Classificadores (using sklearn class objects directly)\n",
        "classifiers_map = {\n",
        "    'SVM': SVC,\n",
        "    'MLP': MLPClassifier,\n",
        "    'RandomForest': RandomForestClassifier,\n",
        "    'KNN': KNeighborsClassifier,\n",
        "    'EnergyBasedFlow': EnergyBasedFlowClassifier,\n",
        "}\n",
        "\n",
        "# --- 1. Extraindo caracteristas dos conjuntos Full Train e Test ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- Initial Feature Extraction (Full Train and Test) ---\")\n",
        "print(\"=\"*60)\n",
        "feature_data = {}\n",
        "\n",
        "for extractor_name, extractor in vit_extractors.items():\n",
        "    print(f\"\\nUsing extractor: {extractor_name}\")\n",
        "    full_train_loader_for_extraction = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
        "    test_loader_for_extraction = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    X_train_full, y_train_full = extractor.extract_features(full_train_loader_for_extraction)\n",
        "    X_test_full, y_test_full = extractor.extract_features(test_loader_for_extraction)\n",
        "\n",
        "    feature_data[extractor_name] = {\n",
        "        'X_train_full': X_train_full,\n",
        "        'y_train_full': y_train_full,\n",
        "        'X_test_full': X_test_full,\n",
        "        'y_test_full': y_test_full,\n",
        "    }\n",
        "    print(f\"Extracted features shape (Train): {X_train_full.shape}\")\n",
        "    print(f\"Extracted features shape (Test): {X_test_full.shape}\")\n",
        "\n",
        "\n",
        "del full_train_loader_for_extraction, test_loader_for_extraction\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# --- 2. K-Fold : Validação cruzada nos dados de treino ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- Starting K-Fold Cross-Validation on Training Data ---\")\n",
        "print(\"=\"*60)\n",
        "kfold_results = []\n",
        "\n",
        "for extractor_name, data in feature_data.items():\n",
        "    X_train_full = data['X_train_full']\n",
        "    y_train_full = data['y_train_full']\n",
        "\n",
        "    print(f\"\\n--- K-Fold for Extractor: {extractor_name} ---\")\n",
        "\n",
        "    kf_split_indices = list(kf.split(X_train_full))\n",
        "\n",
        "\n",
        "    for fold_idx, (train_indices_fold, val_indices_fold) in enumerate(kf_split_indices):\n",
        "        print(f\"\\nProcessing Fold {fold_idx + 1}/{k_folds}\")\n",
        "\n",
        "        X_train_fold = X_train_full[train_indices_fold]\n",
        "        y_train_fold = y_train_full[train_indices_fold]\n",
        "        X_val_fold = X_train_full[val_indices_fold]\n",
        "        y_val_fold = y_train_full[val_indices_fold]\n",
        "\n",
        "        for clf_name, clf_class in classifiers_map.items():\n",
        "            clf_hyperparameters = hyperparameters.get(clf_class.__name__, [])\n",
        "\n",
        "            if not clf_hyperparameters:\n",
        "                 continue\n",
        "\n",
        "            for params in tqdm(clf_hyperparameters, desc=f\"  Fold {fold_idx+1} {clf_name} Params\"):\n",
        "                try:\n",
        "                    clf = clf_class(**params)\n",
        "                    clf.fit(X_train_fold, y_train_fold)\n",
        "                    y_pred_fold = clf.predict(X_val_fold)\n",
        "\n",
        "                    metrics = evaluate_metrics(y_val_fold, y_pred_fold, class_names=class_names, num_classes=num_classes)\n",
        "\n",
        "                    kfold_results.append({\n",
        "                        'extractor': extractor_name,\n",
        "                        'fold_idx': fold_idx + 1,\n",
        "                        'classifier': clf_name,\n",
        "                        'params': params,\n",
        "                        'metrics': metrics\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                     print(f\"\\nError training/predicting {clf_name} with params {params} in fold {fold_idx+1}: {e}\")\n",
        "                     kfold_results.append({\n",
        "                         'extractor': extractor_name,\n",
        "                         'fold_idx': fold_idx + 1,\n",
        "                         'classifier': clf_name,\n",
        "                         'params': params,\n",
        "                         'error': str(e)\n",
        "                     })\n",
        "\n",
        "print(\"\\n--- K-Fold Cross-Validation Finished ---\")\n",
        "print(f\"Total K-Fold results stored (including errors): {len(kfold_results)}\")\n",
        "\n",
        "\n",
        "# --- 3. Analisa os resultados do K-Fold e seleciona os melhores hiperparametros ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- Analyzing K-Fold Results and Selecting Best Models ---\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "aggregated_kfold_metrics = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "params_key_to_dict = {}\n",
        "\n",
        "for result in kfold_results:\n",
        "    if 'error' in result:\n",
        "        continue\n",
        "\n",
        "    extractor = result['extractor']\n",
        "    classifier = result['classifier']\n",
        "\n",
        "    try:\n",
        "         params_key = json.dumps(result['params'], sort_keys=True)\n",
        "         params_key_to_dict[params_key] = result['params']\n",
        "    except TypeError:\n",
        "         params_key = str(result['params'])\n",
        "         params_key_to_dict[params_key] = result['params']\n",
        "\n",
        "\n",
        "    for metric_name, metric_value in result['metrics'].items():\n",
        "\n",
        "        if metric_name not in ['confusion_matrix', 'cm_metrics_per_class']:\n",
        "            if isinstance(metric_value, (int, float)) or (metric_value is None):\n",
        "                 if metric_value is not None:\n",
        "                    aggregated_kfold_metrics[(extractor, classifier, params_key)][metric_name].append(metric_value)\n",
        "\n",
        "\n",
        "average_kfold_metrics = {}\n",
        "best_params_per_clf_extractor = {}\n",
        "\n",
        "selection_metric = 'accuracy'\n",
        "\n",
        "print(f\"Selecting best hyperparameters based on average '{selection_metric}' across folds.\")\n",
        "\n",
        "for (extractor, classifier, params_key), metrics_list_dict in aggregated_kfold_metrics.items():\n",
        "    average_metrics = {\n",
        "        metric_name: np.nanmean(metric_values) if metric_values else np.nan\n",
        "        for metric_name, metric_values in metrics_list_dict.items()\n",
        "    }\n",
        "\n",
        "    average_kfold_metrics[(extractor, classifier, params_key)] = average_metrics\n",
        "\n",
        "    current_best_info = best_params_per_clf_extractor.get((extractor, classifier))\n",
        "    current_combination_score = average_metrics.get(selection_metric, np.nan)\n",
        "\n",
        "    if current_best_info is None or np.isnan(current_best_info.get('avg_score', np.nan)) or (not np.isnan(current_combination_score) and current_combination_score > current_best_info.get('avg_score', np.nan)):\n",
        "        original_params = params_key_to_dict.get(params_key, {})\n",
        "        best_params_per_clf_extractor[(extractor, classifier)] = {\n",
        "            'params': original_params,\n",
        "            'avg_score': current_combination_score\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"\\n--- K-Fold Average Metrics Summary (Best per Classifier/Extractor Combo) ---\")\n",
        "\n",
        "sorted_best_kfold = sorted(best_params_per_clf_extractor.items(),\n",
        "                           key=lambda item: item[1].get('avg_score',\n",
        "                            -float('inf')) if not np.isnan(item[1].get('avg_score',\n",
        "                            -float('inf'))) else -float('inf'),\n",
        "                           reverse=True)\n",
        "\n",
        "for (extractor, classifier), best_info in sorted_best_kfold:\n",
        "     best_params = best_info['params']\n",
        "     avg_score = best_info['avg_score']\n",
        "\n",
        "     try:\n",
        "         best_params_key = json.dumps(best_params, sort_keys=True)\n",
        "     except TypeError:\n",
        "         best_params_key = str(best_params)\n",
        "\n",
        "\n",
        "     full_avg_metrics = average_kfold_metrics.get((extractor, classifier, best_params_key), {})\n",
        "\n",
        "     print(f\"\\nBest K-Fold Result for Extractor: {extractor}, Classifier: {classifier}\")\n",
        "     print(f\"  Selected Params: {best_params}\")\n",
        "     print(f\"  Avg Accuracy (Sample Weighted): {avg_score:.4f}\" if not np.isnan(avg_score) else f\"  Avg Accuracy (Sample Weighted): N/A (all folds failed or metric NaN)\")\n",
        "\n",
        "     print(f\"  Avg Metrics (Macro): Precision={full_avg_metrics.get('precision_macro', np.nan):.4f}, Recall={full_avg_metrics.get('recall_macro', np.nan):.4f}, F1={full_avg_metrics.get('f1_macro', np.nan):.4f}\")\n",
        "     print(f\"  Avg Metrics (Weighted): Precision={full_avg_metrics.get('precision_weighted', np.nan):.4f}, Recall={full_avg_metrics.get('recall_weighted', np.nan):.4f}, F1={full_avg_metrics.get('f1_weighted', np.nan):.4f}\")\n",
        "\n",
        "\n",
        "# --- 4. Avaliação Final no conjunto Test usando o melhores hiperparametros ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- Starting Final Evaluation on Test Set (Using Selected Best Models) ---\")\n",
        "print(\"=\"*60)\n",
        "final_test_results = []\n",
        "\n",
        "for extractor_name, data in feature_data.items():\n",
        "    X_train_full = data['X_train_full']\n",
        "    y_train_full = data['y_train_full']\n",
        "    X_test_full = data['X_test_full']\n",
        "    y_test_full = data['y_test_full']\n",
        "\n",
        "    print(f\"\\n--- Testing with Extractor: {extractor_name} ---\")\n",
        "\n",
        "    for clf_name, clf_class in classifiers_map.items():\n",
        "         best_info = best_params_per_clf_extractor.get((extractor_name, clf_name))\n",
        "\n",
        "         if best_info is None or best_info.get('params') is None or np.isnan(best_info.get('avg_score', np.nan)):\n",
        "              print(f\"  No valid best parameters found for {clf_name} with {extractor_name} (or avg K-Fold score was NaN). Skipping final test.\")\n",
        "\n",
        "              final_test_results.append({\n",
        "                  'extractor': extractor_name,\n",
        "                  'classifier': clf_name,\n",
        "                  'params': best_info.get('params', None) if best_info else None,\n",
        "                  'error': f\"Skipped: No valid best parameters found from K-Fold (avg {selection_metric}={best_info.get('avg_score', np.nan)})\" if best_info else \"Skipped: No best parameters found\"\n",
        "              })\n",
        "              continue\n",
        "\n",
        "         params = best_info['params']\n",
        "         print(f\"  Testing {clf_name} with Selected Best Params: {params}\")\n",
        "\n",
        "         try:\n",
        "            clf = clf_class(**params)\n",
        "            clf.fit(X_train_full, y_train_full)\n",
        "\n",
        "            y_pred_test = clf.predict(X_test_full)\n",
        "\n",
        "            test_metrics = evaluate_metrics(y_test_full, y_pred_test, class_names=class_names, num_classes=num_classes)\n",
        "\n",
        "            final_test_results.append({\n",
        "                'extractor': extractor_name,\n",
        "                'classifier': clf_name,\n",
        "                'params': params,\n",
        "                'metrics': test_metrics,\n",
        "                'y_true': y_test_full.tolist(),\n",
        "                'y_pred': y_pred_test.tolist()\n",
        "            })\n",
        "         except Exception as e:\n",
        "             print(f\"\\nError training/predicting selected {clf_name} with params {params} on test set: {e}\")\n",
        "             final_test_results.append({\n",
        "                 'extractor': extractor_name,\n",
        "                 'classifier': clf_name,\n",
        "                 'params': params,\n",
        "                 'error': str(e)\n",
        "             })\n",
        "\n",
        "del X_train_full, y_train_full\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n--- Final Evaluation on Test Set Finished ---\")\n",
        "print(f\"Total Final Test results stored (selected models): {len(final_test_results)}\")\n",
        "\n",
        "\n",
        "# --- 5. Exibe o resultado final ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- Summarizing Final Test Results (Selected Models) ---\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if not final_test_results:\n",
        "    print(\"No final test results to display.\")\n",
        "else:\n",
        "    for result in final_test_results:\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"Extractor: {result.get('extractor', 'N/A')}\")\n",
        "        print(f\"Classifier: {result.get('classifier', 'N/A')}\")\n",
        "        print(f\"Parameters (Selected by K-Fold): {result.get('params', 'N/A')}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        if 'error' in result:\n",
        "             print(f\"Error during final test: {result['error']}\")\n",
        "             continue\n",
        "\n",
        "        metrics = result['metrics']\n",
        "\n",
        "        print(f\"  Metrics on Test Set:\")\n",
        "        acc_weighted_sample = metrics.get('accuracy')\n",
        "        prec_m = metrics.get('precision_macro')\n",
        "        rec_m = metrics.get('recall_macro')\n",
        "        f1_m = metrics.get('f1_macro')\n",
        "        prec_w = metrics.get('precision_weighted')\n",
        "        rec_w = metrics.get('recall_weighted')\n",
        "        f1_w = metrics.get('f1_weighted')\n",
        "\n",
        "\n",
        "        print(f\"    Accuracy (Sample Weighted): {acc_weighted_sample:.4f}\" if acc_weighted_sample is not None else \"    Accuracy (Sample Weighted): N/A\")\n",
        "        print(f\"    Precision (macro): {prec_m:.4f}\" if prec_m is not None else \"    Precision (macro): N/A\")\n",
        "        print(f\"    Recall (macro): {rec_m:.4f}\" if rec_m is not None else \"    Recall (macro): N/A\")\n",
        "        print(f\"    F1-Score (macro): {f1_m:.4f}\" if f1_m is not None else \"    F1-Score (macro): N/A\")\n",
        "        print(f\"    Precision (weighted): {prec_w:.4f}\" if prec_w is not None else \"    Precision (weighted): N/A\")\n",
        "        print(f\"    Recall (weighted): {rec_w:.4f}\" if rec_w is not None else \"    Recall (weighted): N/A\")\n",
        "        print(f\"    F1-Score (weighted): {f1_w:.4f}\" if f1_w is not None else \"    F1-Score (weighted): N/A\")\n",
        "\n",
        "\n",
        "        cm_list = metrics.get('confusion_matrix')\n",
        "        if cm_list is not None and class_names is not None:\n",
        "            cm_np = np.array(cm_list)\n",
        "\n",
        "            fig_size = max(6, num_classes * 0.7)\n",
        "            plt.figure(figsize=(fig_size, fig_size * 0.8))\n",
        "\n",
        "            sns.heatmap(cm_np,\n",
        "                        annot=True, fmt='d', cmap='Blues',\n",
        "                        xticklabels=class_names, yticklabels=class_names)\n",
        "            plt.xlabel('Predicted Label')\n",
        "            plt.ylabel('True Label')\n",
        "\n",
        "            params_dict_for_title = result.get('params', {})\n",
        "            params_str_title = \", \".join([f\"{k}={v}\" for k, v in params_dict_for_title.items()])\n",
        "            if len(params_str_title) > 50:\n",
        "                params_str_title = params_str_title[:47] + \"...\"\n",
        "\n",
        "            plt.title(f'Confusion Matrix: {result[\"classifier\"]} ({result[\"extractor\"]})\\nParams: {params_str_title}')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        elif cm_list is not None:\n",
        "             print(\"\\n  Confusion Matrix (cannot plot without class names or CM data):\")\n",
        "             print(np.array(cm_list) if cm_list else \"N/A\")\n",
        "        else:\n",
        "             print(\"\\n  Confusion Matrix data not available.\")\n",
        "\n",
        "\n",
        "        cm_metrics_per_class = metrics.get('cm_metrics_per_class')\n",
        "        if cm_metrics_per_class:\n",
        "            print(\"\\n  Per-Class TP/TN/FP/FN on Test Set:\")\n",
        "            sorted_class_labels = sorted(cm_metrics_per_class.keys(), key=lambda x: class_names.index(x) if class_names and x in class_names else x)\n",
        "\n",
        "            for class_label in sorted_class_labels:\n",
        "                cm_vals = cm_metrics_per_class.get(class_label, {})\n",
        "                tp = cm_vals.get('TP', 'N/A')\n",
        "                tn = cm_vals.get('TN', 'N/A')\n",
        "                fp = cm_vals.get('FP', 'N/A')\n",
        "                fn = cm_vals.get('FN', 'N/A')\n",
        "                print(f\"    {class_label}: TP={tp}, TN={tn}, FP={fp}, FN={fn}\")\n",
        "        else:\n",
        "             print(\"\\n  Per-Class metrics not available.\")\n",
        "\n",
        "\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n--- All Final Test Results Displayed ---\")\n",
        "\n",
        "# --- 6. Seleciona e exibe o melhor resultado geral ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- Overall Best Result on Test Set ---\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "best_result_entry = None\n",
        "highest_sample_weighted_accuracy = -1.0\n",
        "\n",
        "for result in final_test_results:\n",
        "\n",
        "    if 'error' in result or result.get('metrics') is None:\n",
        "        continue\n",
        "\n",
        "    current_accuracy = result['metrics'].get('accuracy')\n",
        "\n",
        "    if current_accuracy is not None and not np.isnan(current_accuracy):\n",
        "        if best_result_entry is None or current_accuracy > highest_sample_weighted_accuracy:\n",
        "            highest_sample_weighted_accuracy = current_accuracy\n",
        "            best_result_entry = result\n",
        "\n",
        "if best_result_entry:\n",
        "    print(f\"\\nOverall Best Model configuration based on Test Set Sample-Weighted Accuracy ({highest_sample_weighted_accuracy:.4f}):\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Extractor: {best_result_entry['extractor']}\")\n",
        "    print(f\"Classifier: {best_result_entry['classifier']}\")\n",
        "    print(f\"Parameters (Selected by K-Fold): {best_result_entry['params']}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(\"Metrics on Test Set for the Best Model:\")\n",
        "\n",
        "    metrics = best_result_entry['metrics']\n",
        "    acc_weighted_sample = metrics.get('accuracy')\n",
        "    prec_m = metrics.get('precision_macro')\n",
        "    rec_m = metrics.get('recall_macro')\n",
        "    f1_m = metrics.get('f1_macro')\n",
        "    prec_w = metrics.get('precision_weighted')\n",
        "    rec_w = metrics.get('recall_weighted')\n",
        "    f1_w = metrics.get('f1_weighted')\n",
        "\n",
        "    print(f\"  Accuracy (Sample Weighted): {acc_weighted_sample:.4f}\" if acc_weighted_sample is not None else \"  Accuracy (Sample Weighted): N/A\")\n",
        "    print(f\"  Precision (macro): {prec_m:.4f}\" if prec_m is not None else \"  Precision (macro): N/A\")\n",
        "    print(f\"  Recall (macro): {rec_m:.4f}\" if rec_m is not None else \"  Recall (macro): N/A\")\n",
        "    print(f\"  F1-Score (macro): {f1_m:.4f}\" if f1_m is not None else \"  F1-Score (macro): N/A\")\n",
        "    print(f\"  Precision (weighted): {prec_w:.4f}\" if prec_w is not None else \"  Precision (weighted): N/A\")\n",
        "    print(f\"  Recall (weighted): {rec_w:.4f}\" if rec_w is not None else \"  Recall (weighted): N/A\")\n",
        "    print(f\"  F1-Score (weighted): {f1_w:.4f}\" if f1_w is not None else \"  F1-Score (weighted): N/A\")\n",
        "\n",
        "\n",
        "    cm_list = metrics.get('confusion_matrix')\n",
        "    if cm_list is not None and class_names is not None:\n",
        "        cm_np = np.array(cm_list)\n",
        "\n",
        "        fig_size = max(6, num_classes * 0.7)\n",
        "        plt.figure(figsize=(fig_size, fig_size * 0.8))\n",
        "\n",
        "        sns.heatmap(cm_np,\n",
        "                    annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "\n",
        "        params_dict_for_title = best_result_entry.get('params', {})\n",
        "        params_str_title = \", \".join([f\"{k}={v}\" for k, v in params_dict_for_title.items()])\n",
        "        if len(params_str_title) > 50:\n",
        "            params_str_title = params_str_title[:47] + \"...\"\n",
        "\n",
        "        plt.title(f'Confusion Matrix: {best_result_entry[\"classifier\"]} ({best_result_entry[\"extractor\"]})\\nParams: {params_str_title}\\nTest Set (Best Model)')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    elif cm_list is not None:\n",
        "         print(\"\\n  Confusion Matrix (numerical data for best model):\")\n",
        "         print(np.array(cm_list) if cm_list else \"N/A\")\n",
        "    else:\n",
        "         print(\"\\n  Confusion Matrix data not available for the best model.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Could not determine the overall best result (perhaps all runs failed or had no valid sample-weighted accuracy metric).\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# --- 7. Salva os resutados no arquivo de texto ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"--- Saving Results to File ---\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "output_filename = \"evaluation_results.txt\"\n",
        "\n",
        "try:\n",
        "    with open(output_filename, \"w\") as f:\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        f.write(\"      ViT + Traditional Classifier Evaluation Results\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"=\"*60 + \"\\n\")\n",
        "        f.write(\"--- K-Fold Cross-Validation Summary (Best Params per Combo) ---\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "        if not sorted_best_kfold:\n",
        "            f.write(\"No K-Fold results available.\\n\\n\")\n",
        "        else:\n",
        "            for (extractor, classifier), best_info in sorted_best_kfold:\n",
        "                best_params = best_info['params']\n",
        "                avg_score = best_info['avg_score']\n",
        "\n",
        "                try:\n",
        "                    best_params_key = json.dumps(best_params, sort_keys=True)\n",
        "                except TypeError:\n",
        "                    best_params_key = str(best_params)\n",
        "\n",
        "                full_avg_metrics = average_kfold_metrics.get((extractor, classifier, best_params_key), {})\n",
        "\n",
        "                f.write(f\"Extractor: {extractor}\\n\")\n",
        "                f.write(f\"Classifier: {classifier}\\n\")\n",
        "                f.write(f\"  Selected Params: {best_params}\\n\")\n",
        "                f.write(f\"  Avg Accuracy (Sample Weighted): {avg_score:.4f}\\n\" if not np.isnan(avg_score) else f\"  Avg Accuracy (Sample Weighted): N/A\\n\")\n",
        "\n",
        "                f.write(f\"  Avg Metrics (Macro): Precision={full_avg_metrics.get('precision_macro', np.nan):.4f}, Recall={full_avg_metrics.get('recall_macro', np.nan):.4f}, F1={full_avg_metrics.get('f1_macro', np.nan):.4f}\\n\")\n",
        "                f.write(f\"  Avg Metrics (Weighted): Precision={full_avg_metrics.get('precision_weighted', np.nan):.4f}, Recall={full_avg_metrics.get('recall_weighted', np.nan):.4f}, F1={full_avg_metrics.get('f1_weighted', np.nan):.4f}\\n\")\n",
        "                f.write(\"-\" * 30 + \"\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "\n",
        "        f.write(\"=\"*60 + \"\\n\")\n",
        "        f.write(\"--- Final Evaluation Results on Test Set ---\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "        if not final_test_results:\n",
        "            f.write(\"No final test results available.\\n\\n\")\n",
        "        else:\n",
        "            for i, result in enumerate(final_test_results):\n",
        "                f.write(f\"Result {i+1}:\\n\")\n",
        "                f.write(f\"  Extractor: {result.get('extractor', 'N/A')}\\n\")\n",
        "                f.write(f\"  Classifier: {result.get('classifier', 'N/A')}\\n\")\n",
        "                f.write(f\"  Parameters: {result.get('params', 'N/A')}\\n\")\n",
        "\n",
        "                if 'error' in result:\n",
        "                    f.write(f\"  Status: ERROR - {result['error']}\\n\")\n",
        "                else:\n",
        "                    metrics = result['metrics']\n",
        "                    f.write(\"  Status: Success\\n\")\n",
        "                    f.write(\"  Metrics on Test Set:\\n\")\n",
        "                    acc_weighted_sample = metrics.get('accuracy')\n",
        "                    prec_m = metrics.get('precision_macro')\n",
        "                    rec_m = metrics.get('recall_macro')\n",
        "                    f1_m = metrics.get('f1_macro')\n",
        "                    prec_w = metrics.get('precision_weighted')\n",
        "                    rec_w = metrics.get('recall_weighted')\n",
        "                    f1_w = metrics.get('f1_weighted')\n",
        "\n",
        "                    f.write(f\"    Accuracy (Sample Weighted): {acc_weighted_sample:.4f}\\n\" if acc_weighted_sample is not None else \"    Accuracy (Sample Weighted): N/A\\n\")\n",
        "                    f.write(f\"    Precision (macro): {prec_m:.4f}\\n\" if prec_m is not None else \"    Precision (macro): N/A\\n\")\n",
        "                    f.write(f\"    Recall (macro): {rec_m:.4f}\\n\" if rec_m is not None else \"    Recall (macro): N/A\\n\")\n",
        "                    f.write(f\"    F1-Score (macro): {f1_m:.4f}\\n\" if f1_m is not None else \"    F1-Score (macro): N/A\\n\")\n",
        "                    f.write(f\"    Precision (weighted): {prec_w:.4f}\\n\" if prec_w is not None else \"    Precision (weighted): N/A\\n\")\n",
        "                    f.write(f\"    Recall (weighted): {rec_w:.4f}\\n\" if rec_w is not None else \"    Recall (weighted): N/A\\n\")\n",
        "                    f.write(f\"    F1-Score (weighted): {f1_w:.4f}\\n\" if f1_w is not None else \"    F1-Score (weighted): N/A\\n\")\n",
        "\n",
        "\n",
        "                    cm_list = metrics.get('confusion_matrix')\n",
        "                    if cm_list is not None:\n",
        "                        f.write(\"\\n  Confusion Matrix (Rows: True Label, Columns: Predicted Label):\\n\")\n",
        "                        cm_np = np.array(cm_list)\n",
        "                        f.write(np.array2string(cm_np, separator=', ', threshold=np.inf, precision=0, suppress_small=True) + \"\\n\")\n",
        "                        if class_names is not None:\n",
        "                             f.write(\"  Labels (in order): \" + \", \".join(class_names) + \"\\n\")\n",
        "                        else:\n",
        "                             f.write(\"  Labels: N/A (Class names not available)\\n\")\n",
        "                        f.write(\"  (Plots are generated interactively or saved separately)\\n\")\n",
        "                    else:\n",
        "                        f.write(\"\\n  Confusion Matrix data not available.\\n\")\n",
        "\n",
        "\n",
        "                    cm_metrics_per_class = metrics.get('cm_metrics_per_class')\n",
        "                    if cm_metrics_per_class:\n",
        "                        f.write(\"\\n  Per-Class TP/TN/FP/FN on Test Set:\\n\")\n",
        "                        sorted_class_labels = sorted(cm_metrics_per_class.keys(), key=lambda x: class_names.index(x) if class_names and x in class_names else x)\n",
        "                        for class_label in sorted_class_labels:\n",
        "                            cm_vals = cm_metrics_per_class.get(class_label, {})\n",
        "                            tp = cm_vals.get('TP', 'N/A')\n",
        "                            tn = cm_vals.get('TN', 'N/A')\n",
        "                            fp = cm_vals.get('FP', 'N/A')\n",
        "                            fn = cm_vals.get('FN', 'N/A')\n",
        "                            f.write(f\"    {class_label}: TP={tp}, TN={tn}, FP={fp}, FN={fn}\\n\")\n",
        "                    else:\n",
        "                        f.write(\"\\n  Per-Class metrics not available.\\n\")\n",
        "\n",
        "                f.write(\"-\" * 40 + \"\\n\\n\")\n",
        "\n",
        "        f.write(\"=\"*60 + \"\\n\")\n",
        "        f.write(\"--- Overall Best Result on Test Set (by Sample-Weighted Accuracy) ---\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "        if best_result_entry:\n",
        "            f.write(f\"Overall Best Model configuration based on Test Set Sample-Weighted Accuracy ({highest_sample_weighted_accuracy:.4f}):\\n\")\n",
        "            f.write(\"-\" * 60 + \"\\n\")\n",
        "            f.write(f\"Extractor: {best_result_entry['extractor']}\\n\")\n",
        "            f.write(f\"Classifier: {best_result_entry['classifier']}\\n\")\n",
        "            f.write(f\"Parameters (Selected by K-Fold): {best_result_entry['params']}\\n\")\n",
        "            f.write(\"-\" * 60 + \"\\n\")\n",
        "            f.write(\"Metrics on Test Set for the Best Model:\\n\")\n",
        "\n",
        "            metrics = best_result_entry['metrics']\n",
        "            acc_weighted_sample = metrics.get('accuracy')\n",
        "            prec_m = metrics.get('precision_macro')\n",
        "            rec_m = metrics.get('recall_macro')\n",
        "            f1_m = metrics.get('f1_macro')\n",
        "            prec_w = metrics.get('precision_weighted')\n",
        "            rec_w = metrics.get('recall_weighted')\n",
        "            f1_w = metrics.get('f1_weighted')\n",
        "\n",
        "            f.write(f\"  Accuracy (Sample Weighted): {acc_weighted_sample:.4f}\\n\" if acc_weighted_sample is not None else \"  Accuracy (Sample Weighted): N/A\\n\")\n",
        "            f.write(f\"  Precision (macro): {prec_m:.4f}\\n\" if prec_m is not None else \"  Precision (macro): N/A\\n\")\n",
        "            f.write(f\"  Recall (macro): {rec_m:.4f}\\n\" if rec_m is not None else \"  Recall (macro): N/A\\n\")\n",
        "            f.write(f\"  F1-Score (macro): {f1_m:.4f}\\n\" if f1_m is not None else \"  F1-Score (macro): N/A\\n\")\n",
        "            f.write(f\"  Precision (weighted): {prec_w:.4f}\\n\" if prec_w is not None else \"  Precision (weighted): N/A\\n\")\n",
        "            f.write(f\"  Recall (weighted): {rec_w:.4f}\\n\" if rec_w is not None else \"  Recall (weighted): N/A\\n\")\n",
        "            f.write(f\"  F1-Score (weighted): {f1_w:.4f}\\n\" if f1_w is not None else \"  F1-Score (weighted): N/A\\n\")\n",
        "\n",
        "            cm_list = metrics.get('confusion_matrix')\n",
        "            if cm_list is not None:\n",
        "                f.write(\"\\n  Confusion Matrix (Rows: True Label, Columns: Predicted Label):\\n\")\n",
        "                cm_np = np.array(cm_list)\n",
        "                f.write(np.array2string(cm_np, separator=', ', threshold=np.inf, precision=0, suppress_small=True) + \"\\n\")\n",
        "                if class_names is not None:\n",
        "                    f.write(\"  Labels (in order): \" + \", \".join(class_names) + \"\\n\")\n",
        "                else:\n",
        "                    f.write(\"  Labels: N/A (Class names not available)\\n\")\n",
        "                f.write(\"  (Plot generated interactively or saved separately)\\n\")\n",
        "            else:\n",
        "                f.write(\"\\n  Confusion Matrix data not available.\\n\")\n",
        "\n",
        "\n",
        "            cm_metrics_per_class = metrics.get('cm_metrics_per_class')\n",
        "            if cm_metrics_per_class:\n",
        "                f.write(\"\\n  Per-Class TP/TN/FP/FN on Test Set:\\n\")\n",
        "                sorted_class_labels = sorted(cm_metrics_per_class.keys(), key=lambda x: class_names.index(x) if class_names and x in class_names else x)\n",
        "                for class_label in sorted_class_labels:\n",
        "                    cm_vals = cm_metrics_per_class.get(class_label, {})\n",
        "                    tp = cm_vals.get('TP', 'N/A')\n",
        "                    tn = cm_vals.get('TN', 'N/A')\n",
        "                    fp = cm_vals.get('FP', 'N/A')\n",
        "                    fn = cm_vals.get('FN', 'N/A')\n",
        "                    f.write(f\"    {class_label}: TP={tp}, TN={tn}, FP={fp}, FN={fn}\\n\")\n",
        "            else:\n",
        "                f.write(\"\\n  Per-Class metrics not available.\\n\")\n",
        "\n",
        "        else:\n",
        "            f.write(\"Overall best result could not be determined.\\n\")\n",
        "\n",
        "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "        f.write(\"              End of Results\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "    print(f\"Results successfully saved to {output_filename}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError saving results to file {output_filename}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lRuc8_Ge0b_E",
        "outputId": "1071b86b-197b-45b6-ac4e-f7fc2d79099f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando o dispositivo: cuda\n",
            "Downloading dataset...\n",
            "Train dataset size: 2073\n",
            "Test dataset size: 518\n",
            "Classes: ['Anabaena', 'Aphanizomenon', 'Cylindrospermopsis', 'Dolichospermum', 'Microcystis', 'Nostoc', 'Oscillatoria', 'Phormidium', 'Planktothrix', 'Raphidiopsis']\n",
            "Number of classes: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vitb8 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded facebook/dino-vitb8 as AutoModel.\n",
            "Feature extractor model loaded and set to evaluation mode.\n",
            "Loaded google/vit-base-patch16-224 as AutoModel.\n",
            "Feature extractor model loaded and set to evaluation mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-large-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded google/vit-large-patch16-224 as AutoModel.\n",
            "Feature extractor model loaded and set to evaluation mode.\n",
            "\n",
            "============================================================\n",
            "--- Initial Feature Extraction (Full Train and Test) ---\n",
            "============================================================\n",
            "\n",
            "Using extractor: DINO\n",
            "Extracting features for 2073 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 33/33 [00:23<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features for 518 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 9/9 [00:05<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted features shape (Train): (2073, 768)\n",
            "Extracted features shape (Test): (518, 768)\n",
            "\n",
            "Using extractor: ViT-Base\n",
            "Extracting features for 2073 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 33/33 [00:09<00:00,  3.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features for 518 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 9/9 [00:02<00:00,  3.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted features shape (Train): (2073, 768)\n",
            "Extracted features shape (Test): (518, 768)\n",
            "\n",
            "Using extractor: ViT-Large\n",
            "Extracting features for 2073 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 33/33 [00:20<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features for 518 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 9/9 [00:04<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted features shape (Train): (2073, 1024)\n",
            "Extracted features shape (Test): (518, 1024)\n",
            "\n",
            "============================================================\n",
            "--- Starting K-Fold Cross-Validation on Training Data ---\n",
            "============================================================\n",
            "\n",
            "--- K-Fold for Extractor: DINO ---\n",
            "\n",
            "Processing Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Fold 1 SVM Params: 100%|██████████| 5/5 [00:03<00:00,  1.26it/s]\n",
            "  Fold 1 MLP Params: 100%|██████████| 5/5 [00:09<00:00,  1.87s/it]\n",
            "  Fold 1 RandomForest Params: 100%|██████████| 5/5 [00:38<00:00,  7.79s/it]\n",
            "  Fold 1 KNN Params: 100%|██████████| 5/5 [00:00<00:00, 21.93it/s]\n",
            "  Fold 1 EnergyBasedFlow Params: 100%|██████████| 3/3 [00:00<00:00, 17050.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error training/predicting EnergyBasedFlow with params {'n_epochs': 20, 'learning_rate': 0.001, 'verbose': 0} in fold 1: EnergyBasedFlowClassifier.__init__() got an unexpected keyword argument 'n_epochs'\n",
            "\n",
            "Error training/predicting EnergyBasedFlow with params {'n_epochs': 50, 'learning_rate': 0.001, 'verbose': 0} in fold 1: EnergyBasedFlowClassifier.__init__() got an unexpected keyword argument 'n_epochs'\n",
            "\n",
            "Error training/predicting EnergyBasedFlow with params {'n_epochs': 50, 'learning_rate': 0.0005, 'verbose': 0} in fold 1: EnergyBasedFlowClassifier.__init__() got an unexpected keyword argument 'n_epochs'\n",
            "\n",
            "Processing Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Fold 2 SVM Params: 100%|██████████| 5/5 [00:03<00:00,  1.29it/s]\n",
            "  Fold 2 MLP Params: 100%|██████████| 5/5 [00:08<00:00,  1.62s/it]\n",
            "  Fold 2 RandomForest Params:  80%|████████  | 4/5 [00:18<00:04,  4.53s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3040534000.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m                     \u001b[0my_pred_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         tree._fit(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for clf_name, clf_class in classifiers_map.items() :\n",
        "    print (hyperparameters.get(clf_class.__name__, []))"
      ],
      "metadata": {
        "id": "XtMkNdcU2S8o",
        "outputId": "0f2b6608-5bf6-4377-9529-a162f0f320bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'n_epochs': 20, 'learning_rate': 0.001, 'verbose': 0}, {'n_epochs': 50, 'learning_rate': 0.001, 'verbose': 0}, {'n_epochs': 50, 'learning_rate': 0.0005, 'verbose': 0}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ntcALPiO2eWZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}